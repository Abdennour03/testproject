{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Le pr√©sident Emmanuel Macron a visit√© Paris FR!\",\n",
    "    \"J'adore ce produit üçî, il est incroyable !! #top.\",\n",
    "    \"Tesla Inc. üöó a annonc√© une nouvelle voiture √† 50000‚Ç¨ !!!\",\n",
    "    \"RDV demain √† 18h pour la r√©union importante üìÖ.\",\n",
    "    \"Google a publi√© une mise √† jour de son algorithme üîç v2.0.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte nettoy√© 1: le pr√©sident emmanuel macron a visit√© paris fr\n",
      "Texte nettoy√© 2: jadore ce produit  il est incroyable  \n",
      "Texte nettoy√© 3: tesla inc  a annonc√© une nouvelle voiture √†  \n",
      "Texte nettoy√© 4: rdv demain √† h pour la r√©union importante \n",
      "Texte nettoy√© 5: google a publi√© une mise √† jour de son algorithme  v\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def cleaning_texte(texte):\n",
    "    if isinstance(texte, list):  # V√©rifier si le texte est une liste\n",
    "        texte = ' '.join(texte) \n",
    "    texte = texte.lower()  # Convertir en minuscules\n",
    "    texte = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+|\\d+\", \"\", texte)  # Supprimer URL, @mentions, hashtags\n",
    "    texte = re.sub(r\"[^\\w\\s√Ä-√ø]\", \"\", texte)  # Supprimer ponctuation, emojis, chiffres sp√©ciaux\n",
    "    return texte\n",
    "\n",
    "cleaning_text = [cleaning_texte(text) for text in text]\n",
    "\n",
    "for i, cleaned in enumerate(cleaning_text):\n",
    "    print(f\"Texte nettoy√© {i+1}: {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: le pr√©sident emmanuel macron a visit√© paris fr\n",
      "Sentences: ['le pr√©sident emmanuel macron a visit√© paris fr']\n",
      "Tokens: ['le', 'pr√©sident', 'emmanuel', 'macron', 'a', 'visit√©', 'paris', 'fr']\n",
      "--------------------------------------------------\n",
      "Text 2: jadore ce produit  il est incroyable  \n",
      "Sentences: ['jadore ce produit  il est incroyable']\n",
      "Tokens: ['jadore', 'ce', 'produit', 'il', 'est', 'incroyable']\n",
      "--------------------------------------------------\n",
      "Text 3: tesla inc  a annonc√© une nouvelle voiture √†  \n",
      "Sentences: ['tesla inc  a annonc√© une nouvelle voiture √†']\n",
      "Tokens: ['tesla', 'inc', 'a', 'annonc√©', 'une', 'nouvelle', 'voiture', '√†']\n",
      "--------------------------------------------------\n",
      "Text 4: rdv demain √† h pour la r√©union importante \n",
      "Sentences: ['rdv demain √† h pour la r√©union importante']\n",
      "Tokens: ['rdv', 'demain', '√†', 'h', 'pour', 'la', 'r√©union', 'importante']\n",
      "--------------------------------------------------\n",
      "Text 5: google a publi√© une mise √† jour de son algorithme  v\n",
      "Sentences: ['google a publi√© une mise √† jour de son algorithme  v']\n",
      "Tokens: ['google', 'a', 'publi√©', 'une', 'mise', '√†', 'jour', 'de', 'son', 'algorithme', 'v']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "for i, text in enumerate(cleaning_text):\n",
    "    sentences = nltk.sent_tokenize(text)  \n",
    "    tokens = nltk.word_tokenize(text)  \n",
    "    \n",
    "    print(f'Text {i+1}: {text}')\n",
    "    print('Sentences:', sentences)\n",
    "    print('Tokens:', tokens)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pr√©sident', 'emmanuel', 'macron', 'visit√©', 'paris'], ['jadore', 'produit', 'incroyable'], ['tesla', 'inc', 'annonc√©', 'nouvelle', 'voiture'], ['rdv', 'demain', 'r√©union', 'importante'], ['google', 'publi√©', 'mise', 'jour', 'algorithme']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assurez-vous d'avoir t√©l√©charg√© les stopwords et les tokenizers de NLTK\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Stocker les tokens apr√®s traitement\n",
    "tokens_list = []\n",
    "\n",
    "# Appliquer le pr√©traitement sur chaque texte\n",
    "for i,text in enumerate(cleaning_text):\n",
    "    \n",
    "    tokens = word_tokenize(text)  \n",
    "    \n",
    "    tokens_filtered = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    tokens_list.append(tokens_filtered)  # Ajouter √† la liste\n",
    "\n",
    "print(tokens_list)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1 : ['pr√©sident', 'emmanuel', 'macron', 'visiter', 'pari']\n",
      "text 2 : ['jadore', 'produit', 'incroyable']\n",
      "text 3 : ['tesler', 'inc', 'annoncer', 'nouveau', 'voiture']\n",
      "text 4 : ['rendez-vous', 'demain', 'r√©union', 'important']\n",
      "text 5 : ['google', 'publier', 'mise', 'jour', 'algorithme']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "def lemmatiser_texte(texte):\n",
    "    lemmes = []\n",
    "    # bocle of transform the the mot\n",
    "    for mot in texte:\n",
    "        # applide the nlp for transform \n",
    "        doc = nlp(mot)\n",
    "        # choase each mot \n",
    "        for token in doc:\n",
    "            #, applide the new mot after modifie\n",
    "            lemmes.append(token.lemma_)\n",
    "    return lemmes\n",
    "\n",
    "\n",
    "# choase eache linear of the list and \n",
    "for i, text in enumerate(tokens_list):\n",
    "    text_lemma = lemmatiser_texte(text)\n",
    "    print(f'text {i+1} : {text_lemma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pr√©sident', 'NN'), ('emmanuel', 'NN'), ('macron', 'NN'), ('visit√©', 'NN'), ('paris', 'NN')]\n",
      "[('jadore', 'NN'), ('produit', 'NN'), ('incroyable', 'JJ')]\n",
      "[('tesla', 'NN'), ('inc', 'NN'), ('annonc√©', 'NN'), ('nouvelle', 'JJ'), ('voiture', 'NN')]\n",
      "[('rdv', 'NN'), ('demain', 'NN'), ('r√©union', 'NN'), ('importante', 'NN')]\n",
      "[('google', 'NN'), ('publi√©', 'NN'), ('mise', 'VB'), ('jour', 'NN'), ('algorithme', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "for i, text in enumerate(tokens_list):\n",
    "    \n",
    "\n",
    "\n",
    "    tagged_token = pos_tag(text)\n",
    "    print(tagged_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
