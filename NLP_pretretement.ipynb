{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mot non trouv√©.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texte = \"Bonjour, je m'appelle Abdenour.\"\n",
    "motif = r\"Abdenour\"\n",
    "\n",
    "if re.search(motif, texte):\n",
    "    print(\"Mot trouv√© !\")\n",
    "else:\n",
    "    print(\"Mot non trouv√©.\")\n",
    "# new coment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # transforme the eache text to lower\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text) # rmove the link url\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # rmove each ! ? > , ...\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour   voici un lien \n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour !!  Voici un lien https://exemple.com\"\n",
    "cleaning_text = clean_text(text)\n",
    "print(cleaning_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenazation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j', 'aime', 'L', 'aprentissage', 'automatique']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"j'aime L'aprentissage automatique\"\n",
    "tokeniz = RegexpTokenizer(r'\\w+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"j'aime\", \"L'aprentissage\", 'automatique']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"j'aime L'aprentissage automatique\"\n",
    "tokeniz = RegexpTokenizer(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour ', ' Comment ca va ', ' Tout va bien']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"Bonjour ! Comment ca va ? Tout va bien.\"\n",
    "tokeniz = RegexpTokenizer(r'[^.!?]+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3']\n"
     ]
    }
   ],
   "source": [
    "# tokenization des nombre \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"J'ai 2 chats et 3 chiens\"\n",
    "tokeniz = RegexpTokenizer(r'\\d+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Paul', '#MachineLearning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"salut @Paul, as-tu vu #MachineLearning ?\"\n",
    "tokeniz = RegexpTokenizer(r'[@#]\\w+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contactez-moi', 'exemple', 'email.com', 'ou', 'support', 'site.org.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"Contactez-moi a exemple@email.com ou support@site.org.\"\n",
    "tokeniz = RegexpTokenizer(r'[a-zA-Z0-9._%+-]+[a-zA-Z0-9.-]')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.google.com', 'http://site.org']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"Visitez https://www.google.com ou http://site.org pour\"\n",
    "tokeniz = RegexpTokenizer(r'https?://\\S+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['est', 'un', 'langage', 'de', 'programmation', 'populaire']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "text = \"Python est un langage de programmation populaire.\"\n",
    "tokeniz = RegexpTokenizer(r'\\b[a-z]+\\b') # katdi ghir l minuscule am3alme \n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "print(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('methode', 'de', 'comptage'), ('de', 'comptage', '1'), ('comptage', '1', '2'), ('1', '2', 'grammes')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text_tok = \"methode de comptage (1,2)-grammes\"\n",
    "tokeniz = RegexpTokenizer(r'\\w+')\n",
    "sample_word = tokeniz.tokenize(str(text_tok))\n",
    "trigrammes = list(ngrams(sample_word, 3))\n",
    "print(trigrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'method de comptag gramm'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "text = \"methode de comptage grammes\"\n",
    "tokeniz = RegexpTokenizer(r'\\w+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "fra = SnowballStemmer('french')\n",
    "\" \".join(fra.stem(tok) for tok in sample_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lemmasitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the advancing running inside of to calling mice\n",
      "['the', 'advancing', 'running', 'inside', 'of', 'to', 'calling', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatision = WordNetLemmatizer()\n",
    "text = \"we are going to watching movie\"\n",
    "text = \" the advancing running inside of to calling mice\"\n",
    "# words = word_tokenize(text)\n",
    "tokeniz = RegexpTokenizer(r'\\w+')\n",
    "sample_word = tokeniz.tokenize(str(text))\n",
    "lemmtized_words = [lemmatision.lemmatize(word, pos='n') for word in sample_word]\n",
    "print(text)\n",
    "print(lemmtized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "text = \"the cat sleeps on the mat.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "tagged_token = pos_tag(tokens)\n",
    "print(tagged_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m             entities\u001b[38;5;241m.\u001b[39mappend((entity_name, entity_type))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entities  \u001b[38;5;66;03m# Ajout du retour des entit√©s\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntites_nomees\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(named_entity_recognition)  \u001b[38;5;66;03m# Correction du nom de la fonction\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def named_entity_recognition(tokenz):\n",
    "    tagged_words = nltk.pos_tag(tokenz)\n",
    "    tree = nltk.ne_chunk(tagged_words)\n",
    "    entities = []\n",
    "    \n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, nltk.Tree):  # Correction ici, \"tree\" doit √™tre \"nltk.Tree\"\n",
    "            entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            entity_type = subtree.label()\n",
    "            entities.append((entity_name, entity_type))\n",
    "    \n",
    "    return entities  # Ajout du retour des entit√©s\n",
    "\n",
    "df['Entites_nomees'] = df['Token'].apply(named_entity_recognition)  # Correction du nom de la fonction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('french')  # Correction ici\n",
    "print(len(stop_words))\n",
    "print(stop_words[:7])  # Correction de l'affichage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci exemple phrase contenant mots vides.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stopwords_lis = list(stopwords.words('french'))\n",
    "re.sub(r\"(\\s+|^)(\" + r\"|\".join(stopwords_lis)+r\")(\\s+|$\",\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# T√©l√©charger les stopwords si ce n'est pas encore fait\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Charger les stopwords fran√ßais\n",
    "stopwords_lis = stopwords.words('french')\n",
    "\n",
    "# Exemple de texte\n",
    "text = \"Ceci est un exemple de phrase contenant des mots vides.\"\n",
    "\n",
    "# Correction de l'expression r√©guli√®re en ajoutant \\b pour d√©limiter les mots\n",
    "pattern = r\"\\b(\" + r\"|\".join(map(re.escape, stopwords_lis)) + r\")\\b\"\n",
    "\n",
    "# Remplacement des stopwords par un espace\n",
    "text_clean = re.sub(pattern, \" \", text, flags=re.IGNORECASE)\n",
    "\n",
    "# Suppression des espaces en trop\n",
    "text_clean = re.sub(r\"\\s+\", \" \", text_clean).strip()\n",
    "\n",
    "print(text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Le pr√©sident Emmanuel Macron a visit√© Paris FR!\"\n",
    "text2 = \"J'adore ce produit üçî, il est incroyable !! #top.\"\n",
    "text3 = \"Tesla Inc. üöó a annonc√© une nouvelle voiture √† 50000‚Ç¨ !!!\"\n",
    "text4 = \"RDV demain √† 18h pour la r√©union importante üìÖ.\"\n",
    "text5 = \"Google a publi√© une mise √† jour de son algorithme üîç v2.0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le pr√©sident emmanuel macron a visit√© paris fr\n",
      "jadore ce produit  il est incroyable  top\n",
      "tesla inc  a annonc√© une nouvelle voiture √† 50000 \n",
      "rdv demain √† 18h pour la r√©union importante \n",
      "google a publi√© une mise √† jour de son algorithme  v20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clean_text1 = clean(text1)\n",
    "clean_text2 = clean(text2)\n",
    "clean_text3 = clean(text3)\n",
    "clean_text4 = clean(text4)\n",
    "clean_text5 = clean(text5)\n",
    "print(clean_text1)\n",
    "print(clean_text2)\n",
    "print(clean_text3)\n",
    "print(clean_text4)\n",
    "print(clean_text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'pr√©sident', 'emmanuel', 'macron', 'a', 'visit√©', 'paris', 'fr']\n",
      "['jadore', 'ce', 'produit', 'il', 'est', 'incroyable', 'top']\n",
      "['tesla', 'inc', 'a', 'annonc√©', 'une', 'nouvelle', 'voiture', '√†', '50000']\n",
      "['rdv', 'demain', '√†', '18h', 'pour', 'la', 'r√©union', 'importante']\n",
      "['google', 'a', 'publi√©', 'une', 'mise', '√†', 'jour', 'de', 'son', 'algorithme', 'v20']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokeniz = RegexpTokenizer(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "sample_word1 = tokeniz.tokenize(str(clean_text1))\n",
    "sample_word2 = tokeniz.tokenize(str(clean_text2))\n",
    "sample_word3 = tokeniz.tokenize(str(clean_text3))\n",
    "sample_word4 = tokeniz.tokenize(str(clean_text4))\n",
    "sample_word5 = tokeniz.tokenize(str(clean_text5))\n",
    "print(sample_word1)\n",
    "print(sample_word2)\n",
    "print(sample_word3)\n",
    "print(sample_word4)\n",
    "print(sample_word5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens apr√®s nettoyage : ['√©tudiants', 'travaillent', 'dur', 'r√©ussir', 'leurs', 'examen', ',', 'doivent', 'aussi', 'reposer', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# T√©l√©charger les ressources n√©cessaires (si ce n'est pas d√©j√† fait)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Texte d'exemple\n",
    "texte = \"Les √©tudiants travaillent dur pour r√©ussir leurs examens, mais ils doivent aussi se reposer.\"\n",
    "\n",
    "# 1. Tokenization (d√©couper le texte en mots)\n",
    "tokens = word_tokenize(texte.lower())  # Conversion en minuscules + Tokenization\n",
    "\n",
    "# 2. Suppression des Stop Words\n",
    "stop_words = set(stopwords.words('french'))\n",
    "tokens_filtr√©s = [mot for mot in tokens if mot not in stop_words]\n",
    "\n",
    "# 3. Lemmatisation (R√©duction des mots √† leur forme de base)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatis√©s = [lemmatizer.lemmatize(mot) for mot in tokens_filtr√©s]\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"Tokens apr√®s nettoyage :\", tokens_lemmatis√©s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat : [0. 0. 1. 0. 0.]\n",
      "noir : [0. 0. 0. 1. 0.]\n",
      "chat : [0. 0. 1. 0. 0.]\n",
      "blanc : [1. 0. 0. 0. 0.]\n",
      "oiseau : [0. 0. 0. 0. 1.]\n",
      "bleu : [0. 1. 0. 0. 0.]\n",
      "chat : [0. 0. 1. 0. 0.]\n",
      "noir : [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "CORPUS = [\n",
    "    'chat noir',\n",
    "    'chat blanc',\n",
    "    'oiseau bleu',\n",
    "    'chat noir'\n",
    "    \n",
    "]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encodage_one_hot = encoder.fit_transform([[mot] for phrase in CORPUS for mot in phrase.split()])\n",
    "\n",
    "for phrase in CORPUS:\n",
    "    for mot in phrase.split():\n",
    "        print(f'{mot} : {encoder.transform([[mot]])[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freatur_name:\n",
      "['and' 'corpus' 'document' 'first' 'in' 'is' 'one' 'second' 'the' 'third'\n",
      " 'this']\n",
      "Feature Names (Vocabulary):\n",
      "[[0 1 1 2 1 1 0 0 2 0 1]\n",
      " [0 0 2 0 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CORPUS = [\n",
    "   'this is the first document in the first corpus.',\n",
    "   'this document is the second document.',\n",
    "   'and this is the third one.',\n",
    "   'Is this the first document ?'\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the CORPUS\n",
    "X = vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Convert the result to an array\n",
    "freatur_name = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Display the Bag of Words\n",
    "print(\"freatur_name:\")\n",
    "print(freatur_name)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freatur_name:\n",
      "['and' 'and this' 'corpus' 'document' 'document in' 'document is' 'first'\n",
      " 'first corpus' 'first document' 'in' 'in the' 'is' 'is the' 'is this'\n",
      " 'one' 'second' 'second document' 'the' 'the first' 'the second'\n",
      " 'the third' 'third' 'third one' 'this' 'this document' 'this is'\n",
      " 'this the']\n",
      "Feature Names (Vocabulary):\n",
      "[[0 0 1 1 1 0 2 1 1 1 1 1 1 0 0 0 0 2 2 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 2 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
      " [0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CORPUS = [\n",
    "   'this is the first document in the first corpus.',\n",
    "   'this document is the second document.',\n",
    "   'and this is the third one.',\n",
    "   'Is this the first document ?'\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the CORPUS\n",
    "X = vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Convert the result to an array\n",
    "freatur_name = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Display the Bag of Words\n",
    "print(\"freatur_name:\")\n",
    "print(freatur_name)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(X.toarray())\n",
    "\n",
    "\n",
    "\n",
    "freatur_name:\n",
    "['and this' 'document in' 'document is' 'first corpus' 'first document'\n",
    " 'in the' 'is the' 'is this' 'second document' 'the first' 'the second'\n",
    " 'the third' 'third one' 'this document' 'this is' 'this the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freatur_name:\n",
      "['and this' 'document in' 'document is' 'first corpus' 'first document'\n",
      " 'in the' 'is the' 'is this' 'second document' 'the first' 'the second'\n",
      " 'the third' 'third one' 'this document' 'this is' 'this the']\n",
      "Feature Names (Vocabulary):\n",
      "[[0 1 0 1 1 1 1 0 0 2 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CORPUS = [\n",
    "   'this is the first document in the first corpus.',\n",
    "   'this document is the second document.',\n",
    "   'and this is the third one.',\n",
    "   'Is this the first document ?'\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the CORPUS\n",
    "X = vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Convert the result to an array\n",
    "freatur_name = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Display the Bag of Words\n",
    "print(\"freatur_name:\")\n",
    "print(freatur_name)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.39139941 0.24982517 0.61716758 0.39139941 0.20424845\n",
      "  0.         0.         0.4084969  0.         0.20424845]\n",
      " [0.         0.         0.6876236  0.         0.         0.28108867\n",
      "  0.         0.53864762 0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.         0.         0.26710379\n",
      "  0.51184851 0.         0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.         0.46979139 0.58028582 0.         0.38408524\n",
      "  0.         0.         0.38408524 0.         0.38408524]]\n",
      "Feature Names (Vocabulary):\n",
      "['and' 'corpus' 'document' 'first' 'in' 'is' 'one' 'second' 'the' 'third'\n",
      " 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "CORPUS = [\n",
    "   'this is the first document in the first corpus.',\n",
    "   'this document is the second document.',\n",
    "   'and this is the third one.',\n",
    "   'Is this the first document ?'\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the CORPUS\n",
    "X = vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Convert the result to an array\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
